{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_shape=(224, 224, 3), num_classes=1):\n",
    "    \"\"\"Build a modified ResNet50 model with new front layers.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))\n",
    "    \n",
    "    # Freeze the first half of the model\n",
    "    num_layers = len(base_model.layers)\n",
    "    for layer in base_model.layers[:num_layers // 2]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Custom front layers\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "def preprocess_data(train_dir, batch_size=16, target_size=(224, 224)):\n",
    "    \"\"\"Preprocess images using ImageDataGenerator.\"\"\"\n",
    "    datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, rotation_range=20, validation_split=0.3)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    val_generator = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator\n",
    "\n",
    "def train_and_save_model(train_dir, model_path=r'aburas_model\\custom_my_model_gpt.keras', epochs=10, batch_size=16):\n",
    "    \"\"\"Train the model and save it.\"\"\"\n",
    "    train_generator, test_generator = preprocess_data(train_dir, batch_size)\n",
    "    \n",
    "    model = build_model()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // batch_size,\n",
    "    epochs=epochs)\n",
    "    try:\n",
    "        model.save(model_path)\n",
    "        print(f'Model saved to {model_path}')\n",
    "    except:\n",
    "        print(\"Model Not Saved!!\")\n",
    "    finally:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 564 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\LLM Trainer\\.llmenv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\LLM Trainer\\.llmenv\\lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor']\n",
      "Received: inputs=Tensor(shape=(None, 224, 224, 3))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.4838 - loss: 15.2098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\LLM Trainer\\.llmenv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 10s/step - accuracy: 0.4854 - loss: 14.9736 - val_accuracy: 0.5469 - val_loss: 907.6581\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/35\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:11\u001b[0m 9s/step - accuracy: 0.6250 - loss: 1.0348"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\LLM Trainer\\.llmenv\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 424ms/step - accuracy: 0.6250 - loss: 1.0348 - val_accuracy: 0.4766 - val_loss: 120.0404\n",
      "Epoch 3/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 10s/step - accuracy: 0.5841 - loss: 0.7223 - val_accuracy: 0.4609 - val_loss: 7.3131\n",
      "Epoch 4/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 424ms/step - accuracy: 0.6875 - loss: 0.7549 - val_accuracy: 0.4609 - val_loss: 5.9917\n",
      "Epoch 5/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 9s/step - accuracy: 0.6118 - loss: 0.6752 - val_accuracy: 0.4844 - val_loss: 0.6813\n",
      "Epoch 6/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 421ms/step - accuracy: 0.8125 - loss: 0.4847 - val_accuracy: 0.5391 - val_loss: 0.6808\n",
      "Epoch 7/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 9s/step - accuracy: 0.6843 - loss: 0.6391 - val_accuracy: 0.4453 - val_loss: 1.9491\n",
      "Epoch 8/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 432ms/step - accuracy: 0.8125 - loss: 0.4288 - val_accuracy: 0.4844 - val_loss: 1.9119\n",
      "Epoch 9/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 9s/step - accuracy: 0.6838 - loss: 0.6401 - val_accuracy: 0.4531 - val_loss: 0.6950\n",
      "Epoch 10/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 412ms/step - accuracy: 0.8125 - loss: 0.5320 - val_accuracy: 0.4844 - val_loss: 0.6951\n",
      "Model Not Saved!!\n"
     ]
    }
   ],
   "source": [
    "train_directory = \"..//COVID Project\"  # Replace with actual dataset path\n",
    "model = train_and_save_model(train_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with actual dataset path\n",
    "train_and_save_model(train_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_directory = \"path_to_training_data\"  # Replace with actual dataset path\n",
    "    train_and_save_model(train_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
